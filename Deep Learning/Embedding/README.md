## 🧠 Text Embedding with Sentence Transformers
This project demonstrates how to convert natural language sentences into high-dimensional vector representations (embeddings) using Sentence Transformers. These embeddings capture semantic meaning and are crucial for downstream tasks like semantic search, clustering, similarity matching, and more.

### 📌 Objective
To illustrate the process of generating and visualizing embeddings from a set of text inputs using pretrained transformer models.

### 📓 Notebook Overview
- The Jupyter notebook Embed.ipynb covers:
- Importing Libraries
- Loading SentenceTransformer Model
- Embedding a List of Sentences
- Visualizing Embeddings (First 10 values per sentence)
- Understanding Semantic Similarity

### 🧪 Embedding Pipeline
- Model Used: all-MiniLM-L6-v2 (via sentence-transformers)
- Output: A dense vector (embedding) of size 384 per sentence
- Use Case: Semantic similarity search (e.g., finding that sentence 1 and 2 are semantically close)

 ### 📈 Applications of Text Embeddings
- 🔎 Semantic Search (e.g., RAG, FAQ bots)
- 🔗 Text Clustering & Classification
- 🔁 Duplicate Detection
- 🤖 Information Retrieval for LLMs

### 🛠️ Tech Stack
- Python
- SentenceTransformers
- Jupyter Notebook

 ### 🧑‍💻 Author
Developed by [Raksha](https://github.com/Rakshaa-17)

Let's connect [LinkedIn](https://www.linkedin.com/in/rakshamalela/)
